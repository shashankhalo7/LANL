{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(7)\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.95693302154541\n"
     ]
    }
   ],
   "source": [
    "then = time.time()\n",
    "df = pd.read_csv('data/train/train.csv',dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float64} ) # float32 is enough :)\n",
    "now = time.time()\n",
    "print(now-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf = df['time_to_failure'].values\n",
    "index_start = np.nonzero(np.diff(ttf) > 0)[0] + 1\n",
    "index_start = np.insert(index_start, 0, 0)\n",
    "dict_df={}\n",
    "for i in range(len(index_start)):\n",
    "    if i<(len(index_start)-1):\n",
    "        df_tmp=df[index_start[i]:index_start[i+1]]\n",
    "    else:\n",
    "        df_tmp=df[index_start[i]:]       \n",
    "    dict_df[\"df\"+str(i)]=df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return random.randint(0, l - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrainingExample(df_dict):\n",
    "    k = len(df_dict)\n",
    "    num=randomChoice(k)\n",
    "    #print(num)\n",
    "    len_df=len(df_dict['df'+str(num)])\n",
    "    idx_start=random.randint(0,len_df-149999)\n",
    "    idx_end=idx_start+150000\n",
    "    #print('Start Index:',idx_start)\n",
    "    #print('End Index:',idx_end)\n",
    "    df_tmp=df_dict['df'+str(num)]\n",
    "    sample_x=df_tmp.iloc[idx_start:idx_end]['acoustic_data']\n",
    "    sample_y=df_tmp.iloc[idx_start:idx_end]['time_to_failure']\n",
    "\n",
    "    sample_x1=np.diff(sample_x)\n",
    "    sample_y1=np.diff(sample_y)\n",
    "    meanx1=np.mean(sample_x1)\n",
    "    meany1=np.mean(sample_y1)\n",
    "    sample_x1=np.append(sample_x1,meanx1)\n",
    "    sample_y1=np.append( sample_y1,meany1)\n",
    "\n",
    "    sample_x2=np.diff(sample_x1)\n",
    "    sample_y2=np.diff(sample_y1)\n",
    "    meanx2=np.mean(sample_x2)\n",
    "    meany2=np.mean(sample_y2)\n",
    "    sample_x2=np.append(sample_x2,meanx2)\n",
    "    sample_y2=np.append(sample_y2,meany2)\n",
    "  #   sample_y2=np.append(sample_y2,meany2)\n",
    "\n",
    "    sample_x=np.array(sample_x)\n",
    "    sample_y=np.array(sample_y)\n",
    "    sample_x1=np.array(sample_x1)\n",
    "    sample_y1=np.array(sample_y1)\n",
    "    sample_x2=np.array(sample_x2)\n",
    "    sample_y2=np.array(sample_y2)\n",
    "\n",
    "    #print(sample_x.shape)\n",
    "    #print(sample_x1.shape)\n",
    "    #print(sample_x2.shape)\n",
    "    xtable= [[ 0 for i in range(300) ] for j in range(500)]\n",
    "    ytable= [[ 0 for i in range(300) ] for j in range(500)]\n",
    "    for i in range(300):\n",
    "        for j in range(500):\n",
    "            x=[]\n",
    "            x1=[]\n",
    "            x2=[]\n",
    "            x.append(sample_x[500*i+j])\n",
    "            x1.append(sample_x1[500*i+j])\n",
    "            x2.append(sample_x2[500*i+j])\n",
    "            xtable[j][i]=x+x1+x2\n",
    "    for i in range(300):\n",
    "        for j in range(500):\n",
    "            x=[]\n",
    "            x1=[]\n",
    "            x2=[]\n",
    "            x.append(sample_y[500*i+j])\n",
    "            #x1.append(sample_y1[500*i+j])\n",
    "            #x2.append(sample_y2[500*i+j])\n",
    "            ytable[j][i]=x\n",
    "    uid =str(num)+'-'+str(idx_start)+'-'+str(idx_end)\n",
    "    return uid,xtable,ytable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid,xtrain,ytrain=randomTrainingExample(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-46923717-47073717'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a = np.array(xtrain)\n",
    "#a[0][0]\n",
    "uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=np.array(xtrain)\n",
    "ytrain=np.array(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain= xtrain.astype(np.float32)\n",
    "ytrain= ytrain.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.0000000e+00, -3.0000000e+00,  9.0000000e+00],\n",
       "        [ 5.0000000e+00,  0.0000000e+00,  3.0000000e+00],\n",
       "        [ 1.0000000e+00,  0.0000000e+00, -4.0000000e+00],\n",
       "        ...,\n",
       "        [ 6.0000000e+00,  1.0000000e+00, -4.0000000e+00],\n",
       "        [ 7.0000000e+00, -6.0000000e+00,  8.0000000e+00],\n",
       "        [ 5.0000000e+00,  3.0000000e+00, -9.0000000e+00]],\n",
       "\n",
       "       [[-6.0000000e+00,  6.0000000e+00, -8.0000000e+00],\n",
       "        [ 5.0000000e+00,  3.0000000e+00, -5.0000000e+00],\n",
       "        [ 1.0000000e+00, -4.0000000e+00,  3.0000000e+00],\n",
       "        ...,\n",
       "        [ 7.0000000e+00, -3.0000000e+00,  9.0000000e+00],\n",
       "        [ 1.0000000e+00,  2.0000000e+00,  3.0000000e+00],\n",
       "        [ 8.0000000e+00, -6.0000000e+00,  6.0000000e+00]],\n",
       "\n",
       "       [[ 0.0000000e+00, -2.0000000e+00,  2.0000000e+00],\n",
       "        [ 8.0000000e+00, -2.0000000e+00,  5.0000000e+00],\n",
       "        [-3.0000000e+00, -1.0000000e+00,  3.0000000e+00],\n",
       "        ...,\n",
       "        [ 4.0000000e+00,  6.0000000e+00, -1.0000000e+01],\n",
       "        [ 3.0000000e+00,  5.0000000e+00, -1.0000000e+01],\n",
       "        [ 2.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.0000000e+00,  3.0000000e+00, -6.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "        [ 1.1000000e+01, -5.0000000e+00,  4.0000000e+00],\n",
       "        ...,\n",
       "        [ 2.0000000e+00,  5.0000000e+00, -7.0000000e+00],\n",
       "        [ 6.0000000e+00, -2.0000000e+00,  0.0000000e+00],\n",
       "        [ 4.0000000e+00, -3.0000000e+00,  3.0000000e+00]],\n",
       "\n",
       "       [[ 1.2000000e+01, -3.0000000e+00, -1.0000000e+00],\n",
       "        [ 0.0000000e+00,  1.0000000e+00, -1.0000000e+00],\n",
       "        [ 6.0000000e+00, -1.0000000e+00,  5.0000000e+00],\n",
       "        ...,\n",
       "        [ 7.0000000e+00, -2.0000000e+00,  4.0000000e+00],\n",
       "        [ 4.0000000e+00, -2.0000000e+00,  5.0000000e+00],\n",
       "        [ 1.0000000e+00,  0.0000000e+00,  2.6666845e-05]],\n",
       "\n",
       "       [[ 9.0000000e+00, -4.0000000e+00,  4.0000000e+00],\n",
       "        [ 1.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 5.0000000e+00,  4.0000000e+00, -6.0000000e+00],\n",
       "        ...,\n",
       "        [ 5.0000000e+00,  2.0000000e+00, -8.0000000e+00],\n",
       "        [ 2.0000000e+00,  3.0000000e+00,  0.0000000e+00],\n",
       "        [ 1.0000000e+00,  2.6666845e-05,  2.0000311e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "learning_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainimg=img_transform(xtrain)\n",
    "ytrainimg=img_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_final_x={}\n",
    "dict_final_y={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 11.167648553848267\n"
     ]
    }
   ],
   "source": [
    "n_examples=17\n",
    "now=time.time()\n",
    "while(n_examples):\n",
    "    while(uid in dict_final_x.keys()):\n",
    "        uid,xtrain,ytrain=randomTrainingExample(dict_df)\n",
    "    xtrain=np.array(xtrain)\n",
    "    ytrain=np.array(ytrain)\n",
    "    xtrain= xtrain.astype(np.float64)\n",
    "    ytrain= ytrain.astype(np.float64)\n",
    "    dict_final_x[uid]=xtrainimg\n",
    "    dict_final_y[uid]=ytrainimg\n",
    "    n_examples-=1\n",
    "then=time.time()\n",
    "print(\"Time Taken:\",then-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 500, 300])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_final_x[uid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=3, padding=(2,0)),  # b,16, 168, 100\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 84, 50\n",
    "            nn.Conv2d(16, 8, 3, stride=3, padding=(0,2)),  # b, 8,28 , 18\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2)  # b, 8, 14, 9\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 29, 19\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 87, 57\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 3, 2, stride=2, padding=1),  # b, 3,172, 112\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(3, 1, 1, stride=3, padding=(7,17)),  # b, 1, 500, 300\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the custom dataloader\n",
    "def get_x(uid_list):\n",
    "    for i,uid in enumerate(uid_list):\n",
    "        x=dict_final_x[uid]\n",
    "        x=x.view(1,x.shape[0],x.shape[1],x.shape[2])\n",
    "        if(i==0):\n",
    "            out=x\n",
    "        else:\n",
    "            out = torch.cat((out, x), 0)\n",
    "    return out\n",
    "        \n",
    "def get_y(uid_list):\n",
    "    for i,uid in enumerate(uid_list):\n",
    "        y=dict_final_y[uid]\n",
    "        y=y.view(1,y.shape[0],y.shape[1],y.shape[2])\n",
    "        if(i==0):\n",
    "            out=y\n",
    "        else:\n",
    "            out = torch.cat((out, y), 0)\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(list(dict_final_x.keys()), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200], loss:14.4167\n",
      "epoch [2/200], loss:9.2727\n",
      "epoch [3/200], loss:9.2744\n",
      "epoch [4/200], loss:8.6980\n",
      "epoch [5/200], loss:8.7110\n",
      "epoch [6/200], loss:8.7267\n",
      "epoch [7/200], loss:8.7717\n",
      "epoch [8/200], loss:8.5887\n",
      "epoch [9/200], loss:9.2255\n",
      "epoch [10/200], loss:9.2048\n",
      "epoch [11/200], loss:9.1888\n",
      "epoch [12/200], loss:9.2753\n",
      "epoch [13/200], loss:19.9739\n",
      "epoch [14/200], loss:19.8310\n",
      "epoch [15/200], loss:9.5886\n",
      "epoch [16/200], loss:8.7191\n",
      "epoch [17/200], loss:8.8064\n",
      "epoch [18/200], loss:8.9585\n",
      "epoch [19/200], loss:10.0423\n",
      "epoch [20/200], loss:9.4628\n",
      "epoch [21/200], loss:9.2669\n",
      "epoch [22/200], loss:9.1810\n",
      "epoch [23/200], loss:9.0844\n",
      "epoch [24/200], loss:8.8051\n",
      "epoch [25/200], loss:8.4066\n",
      "epoch [26/200], loss:8.5031\n",
      "epoch [27/200], loss:8.4946\n",
      "epoch [28/200], loss:8.8274\n",
      "epoch [29/200], loss:11.4920\n",
      "epoch [30/200], loss:8.9512\n",
      "epoch [31/200], loss:8.4569\n",
      "epoch [32/200], loss:8.4131\n",
      "epoch [33/200], loss:8.4080\n",
      "epoch [34/200], loss:8.4071\n",
      "epoch [35/200], loss:8.8964\n",
      "epoch [36/200], loss:9.6617\n",
      "epoch [37/200], loss:9.7906\n",
      "epoch [38/200], loss:9.7973\n",
      "epoch [39/200], loss:9.8616\n",
      "epoch [40/200], loss:9.8516\n",
      "epoch [41/200], loss:8.9698\n",
      "epoch [42/200], loss:9.1663\n",
      "epoch [43/200], loss:8.9621\n",
      "epoch [44/200], loss:8.5697\n",
      "epoch [45/200], loss:8.4529\n",
      "epoch [46/200], loss:8.4244\n",
      "epoch [47/200], loss:8.5371\n",
      "epoch [48/200], loss:9.4670\n",
      "epoch [49/200], loss:9.7733\n",
      "epoch [50/200], loss:9.7251\n",
      "epoch [51/200], loss:9.5639\n",
      "epoch [52/200], loss:9.1150\n",
      "epoch [53/200], loss:8.7947\n",
      "epoch [54/200], loss:8.8251\n",
      "epoch [55/200], loss:8.6747\n",
      "epoch [56/200], loss:8.5390\n",
      "epoch [57/200], loss:8.4722\n",
      "epoch [58/200], loss:8.4534\n",
      "epoch [59/200], loss:9.2267\n",
      "epoch [60/200], loss:9.0912\n",
      "epoch [61/200], loss:8.8792\n",
      "epoch [62/200], loss:8.5597\n",
      "epoch [63/200], loss:8.6395\n",
      "epoch [64/200], loss:8.6254\n",
      "epoch [65/200], loss:8.7446\n",
      "epoch [66/200], loss:8.5687\n",
      "epoch [67/200], loss:8.5134\n",
      "epoch [68/200], loss:8.8241\n",
      "epoch [69/200], loss:8.5093\n",
      "epoch [70/200], loss:8.4935\n",
      "epoch [71/200], loss:8.4872\n",
      "epoch [72/200], loss:8.4933\n",
      "epoch [73/200], loss:8.5181\n",
      "epoch [74/200], loss:8.5640\n",
      "epoch [75/200], loss:8.5816\n",
      "epoch [76/200], loss:9.1098\n",
      "epoch [77/200], loss:8.5084\n",
      "epoch [78/200], loss:8.4687\n",
      "epoch [79/200], loss:8.4573\n",
      "epoch [80/200], loss:8.4622\n",
      "epoch [81/200], loss:8.4796\n",
      "epoch [82/200], loss:8.5084\n",
      "epoch [83/200], loss:10.1323\n",
      "epoch [84/200], loss:10.0001\n",
      "epoch [85/200], loss:9.9432\n",
      "epoch [86/200], loss:8.5492\n",
      "epoch [87/200], loss:8.5016\n",
      "epoch [88/200], loss:8.4794\n",
      "epoch [89/200], loss:8.4714\n",
      "epoch [90/200], loss:8.4762\n",
      "epoch [91/200], loss:8.4959\n",
      "epoch [92/200], loss:8.5347\n",
      "epoch [93/200], loss:8.5730\n",
      "epoch [94/200], loss:8.5991\n",
      "epoch [95/200], loss:8.5592\n",
      "epoch [96/200], loss:8.5150\n",
      "epoch [97/200], loss:8.4874\n",
      "epoch [98/200], loss:8.4781\n",
      "epoch [99/200], loss:9.1728\n",
      "epoch [100/200], loss:8.4716\n",
      "epoch [101/200], loss:8.4787\n",
      "epoch [102/200], loss:8.5032\n",
      "epoch [103/200], loss:8.5432\n",
      "epoch [104/200], loss:8.5584\n",
      "epoch [105/200], loss:8.5657\n",
      "epoch [106/200], loss:8.5434\n",
      "epoch [107/200], loss:8.5155\n",
      "epoch [108/200], loss:8.5046\n",
      "epoch [109/200], loss:8.4997\n",
      "epoch [110/200], loss:8.4998\n",
      "epoch [111/200], loss:8.5093\n",
      "epoch [112/200], loss:8.5249\n",
      "epoch [113/200], loss:9.0676\n",
      "epoch [114/200], loss:8.4706\n",
      "epoch [115/200], loss:8.4509\n",
      "epoch [116/200], loss:8.4519\n",
      "epoch [117/200], loss:8.4660\n",
      "epoch [118/200], loss:8.8474\n",
      "epoch [119/200], loss:8.6347\n",
      "epoch [120/200], loss:8.6004\n",
      "epoch [121/200], loss:8.6142\n",
      "epoch [122/200], loss:8.5202\n",
      "epoch [123/200], loss:8.4826\n",
      "epoch [124/200], loss:8.5552\n",
      "epoch [125/200], loss:8.4717\n",
      "epoch [126/200], loss:8.9120\n",
      "epoch [127/200], loss:8.5122\n",
      "epoch [128/200], loss:9.3074\n",
      "epoch [129/200], loss:8.5850\n",
      "epoch [130/200], loss:8.5491\n",
      "epoch [131/200], loss:8.5184\n",
      "epoch [132/200], loss:8.5074\n",
      "epoch [133/200], loss:8.5112\n",
      "epoch [134/200], loss:8.9190\n",
      "epoch [135/200], loss:8.8447\n",
      "epoch [136/200], loss:9.5013\n",
      "epoch [137/200], loss:8.8372\n",
      "epoch [138/200], loss:8.6932\n",
      "epoch [139/200], loss:8.5317\n",
      "epoch [140/200], loss:8.5321\n",
      "epoch [141/200], loss:8.8428\n",
      "epoch [142/200], loss:8.7976\n",
      "epoch [143/200], loss:8.6825\n",
      "epoch [144/200], loss:8.5353\n",
      "epoch [145/200], loss:8.5422\n",
      "epoch [146/200], loss:8.5250\n",
      "epoch [147/200], loss:8.9392\n",
      "epoch [148/200], loss:8.5361\n",
      "epoch [149/200], loss:10.2267\n",
      "epoch [150/200], loss:10.2318\n",
      "epoch [151/200], loss:9.8101\n",
      "epoch [152/200], loss:8.5297\n",
      "epoch [153/200], loss:8.5692\n",
      "epoch [154/200], loss:8.5145\n",
      "epoch [155/200], loss:8.5039\n",
      "epoch [156/200], loss:8.5056\n",
      "epoch [157/200], loss:8.5158\n",
      "epoch [158/200], loss:10.2550\n",
      "epoch [159/200], loss:8.6534\n",
      "epoch [160/200], loss:8.5131\n",
      "epoch [161/200], loss:8.4932\n",
      "epoch [162/200], loss:8.4857\n",
      "epoch [163/200], loss:8.4935\n",
      "epoch [164/200], loss:8.5119\n",
      "epoch [165/200], loss:8.5462\n",
      "epoch [166/200], loss:8.5648\n",
      "epoch [167/200], loss:8.5540\n",
      "epoch [168/200], loss:8.5280\n",
      "epoch [169/200], loss:8.5001\n",
      "epoch [170/200], loss:8.4939\n",
      "epoch [171/200], loss:9.6907\n",
      "epoch [172/200], loss:9.4538\n",
      "epoch [173/200], loss:9.3426\n",
      "epoch [174/200], loss:8.6462\n",
      "epoch [175/200], loss:8.5567\n",
      "epoch [176/200], loss:8.5313\n",
      "epoch [177/200], loss:8.5056\n",
      "epoch [178/200], loss:9.1027\n",
      "epoch [179/200], loss:8.8338\n",
      "epoch [180/200], loss:8.5492\n",
      "epoch [181/200], loss:8.5822\n",
      "epoch [182/200], loss:8.5536\n",
      "epoch [183/200], loss:8.5461\n",
      "epoch [184/200], loss:8.5470\n",
      "epoch [185/200], loss:8.7176\n",
      "epoch [186/200], loss:8.4809\n",
      "epoch [187/200], loss:8.4822\n",
      "epoch [188/200], loss:8.5022\n",
      "epoch [189/200], loss:8.6058\n",
      "epoch [190/200], loss:8.5532\n",
      "epoch [191/200], loss:8.8928\n",
      "epoch [192/200], loss:8.5246\n",
      "epoch [193/200], loss:8.5096\n",
      "epoch [194/200], loss:8.5044\n",
      "epoch [195/200], loss:8.9739\n",
      "epoch [196/200], loss:8.5140\n",
      "epoch [197/200], loss:8.5699\n",
      "epoch [198/200], loss:8.5035\n",
      "epoch [199/200], loss:8.6827\n",
      "epoch [200/200], loss:9.1706\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img = get_x(data)\n",
    "        img_out=get_y(data)\n",
    "        img_out=img_out.cuda()\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img_out)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainy_17000_3channel.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_final_y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9dbe3ce9b99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_final_y\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "with open('trainy_17000_3channel.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(dict_final_y==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18700"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_final_y.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18700"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
