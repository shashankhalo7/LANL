{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(7)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.52645611763\n"
     ]
    }
   ],
   "source": [
    "then = time.time()\n",
    "df = pd.read_csv('data/train/train.csv',dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float64} ) # float32 is enough :)\n",
    "now = time.time()\n",
    "print(now-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf = df['time_to_failure'].values\n",
    "index_start = np.nonzero(np.diff(ttf) > 0)[0] + 1\n",
    "index_start = np.insert(index_start, 0, 0)\n",
    "dict_df={}\n",
    "for i in range(len(index_start)):\n",
    "    if i<(len(index_start)-1):\n",
    "        df_tmp=df[index_start[i]:index_start[i+1]]\n",
    "    else:\n",
    "        df_tmp=df[index_start[i]:]       \n",
    "    dict_df[\"df\"+str(i)]=df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return random.randint(0, l - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrainingExample(df_dict):\n",
    "    k = len(df_dict)\n",
    "    num=randomChoice(k)\n",
    "    #print(num)\n",
    "    len_df=len(df_dict['df'+str(num)])\n",
    "    idx_start=random.randint(0,len_df-149999)\n",
    "    idx_end=idx_start+150000\n",
    "    #print('Start Index:',idx_start)\n",
    "    #print('End Index:',idx_end)\n",
    "    df_tmp=df_dict['df'+str(num)]\n",
    "    sample_x=df_tmp.iloc[idx_start:idx_end]['acoustic_data']\n",
    "    sample_y=df_tmp.iloc[idx_start:idx_end]['time_to_failure']\n",
    "\n",
    "    sample_x1=np.diff(sample_x)\n",
    "    sample_y1=np.diff(sample_y)\n",
    "    meanx1=np.mean(sample_x1)\n",
    "    meany1=np.mean(sample_y1)\n",
    "    sample_x1=np.append(sample_x1,meanx1)\n",
    "    sample_y1=np.append( sample_y1,meany1)\n",
    "\n",
    "    sample_x2=np.diff(sample_x1)\n",
    "    sample_y2=np.diff(sample_y1)\n",
    "    meanx2=np.mean(sample_x2)\n",
    "    meany2=np.mean(sample_y2)\n",
    "    sample_x2=np.append(sample_x2,meanx2)\n",
    "    sample_y2=np.append(sample_y2,meany2)\n",
    "  #   sample_y2=np.append(sample_y2,meany2)\n",
    "\n",
    "    sample_x=np.array(sample_x)\n",
    "    sample_y=np.array(sample_y)\n",
    "    sample_x1=np.array(sample_x1)\n",
    "    sample_y1=np.array(sample_y1)\n",
    "    sample_x2=np.array(sample_x2)\n",
    "    sample_y2=np.array(sample_y2)\n",
    "\n",
    "    #print(sample_x.shape)\n",
    "    #print(sample_x1.shape)\n",
    "    #print(sample_x2.shape)\n",
    "    xtable= [[ 0 for i in range(100) ] for j in range(1500)]\n",
    "    ytable= [[ 0 for i in range(100) ] for j in range(1500)]\n",
    "    for i in range(100):\n",
    "        for j in range(1500):\n",
    "            x=[]\n",
    "            x1=[]\n",
    "            x2=[]\n",
    "            x.append(sample_x[1500*i+j])\n",
    "            x1.append(sample_x1[1500*i+j])\n",
    "            x2.append(sample_x2[1500*i+j])\n",
    "            xtable[j][i]=x+x1+x2\n",
    "    for i in range(100):\n",
    "        for j in range(1500):\n",
    "            x=[]\n",
    "            x1=[]\n",
    "            x2=[]\n",
    "            x.append(sample_y[1500*i+j])\n",
    "            #x1.append(sample_y1[1500*i+j])\n",
    "            #x2.append(sample_y2[1500*i+j])\n",
    "            ytable[j][i]=x\n",
    "    uid =str(num)+'-'+str(idx_start)+'-'+str(idx_end)\n",
    "    return uid,xtable,ytable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid,xtrain,ytrain=randomTrainingExample(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-29101469-29251469'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a = np.array(xtrain)\n",
    "#a[0][0]\n",
    "uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=np.array(xtrain)\n",
    "ytrain=np.array(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain= xtrain.astype(np.float32)\n",
    "ytrain= ytrain.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.0000000e+00,  2.0000000e+00, -1.0000000e+00],\n",
       "        [ 3.0000000e+00, -1.0000000e+00,  5.0000000e+00],\n",
       "        [ 5.0000000e+00, -2.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 3.0000000e+00, -4.0000000e+00,  9.0000000e+00],\n",
       "        [ 2.0000000e+00, -6.0000000e+00,  1.0000000e+01],\n",
       "        [ 8.0000000e+00,  2.0000000e+00, -3.0000000e+00]],\n",
       "\n",
       "       [[ 5.0000000e+00,  1.0000000e+00, -2.0000000e+00],\n",
       "        [ 2.0000000e+00,  4.0000000e+00, -2.0000000e+00],\n",
       "        [ 3.0000000e+00, -2.0000000e+00,  3.0000000e+00],\n",
       "        ...,\n",
       "        [-1.0000000e+00,  5.0000000e+00, -4.0000000e+00],\n",
       "        [-4.0000000e+00,  4.0000000e+00, -8.0000000e+00],\n",
       "        [ 1.0000000e+01, -1.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 6.0000000e+00, -1.0000000e+00, -4.0000000e+00],\n",
       "        [ 6.0000000e+00,  2.0000000e+00, -3.0000000e+00],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  4.0000000e+00],\n",
       "        ...,\n",
       "        [ 4.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
       "        [ 0.0000000e+00, -4.0000000e+00,  3.0000000e+00],\n",
       "        [ 9.0000000e+00, -1.0000000e+00, -3.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.0000000e+00, -3.0000000e+00,  5.0000000e+00],\n",
       "        [ 2.0000000e+00,  0.0000000e+00, -2.0000000e+00],\n",
       "        [ 7.0000000e+00, -6.0000000e+00,  6.0000000e+00],\n",
       "        ...,\n",
       "        [ 1.3000000e+01,  0.0000000e+00, -2.0000000e+00],\n",
       "        [ 6.0000000e+00, -3.0000000e+00,  6.0000000e+00],\n",
       "        [ 4.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 4.0000000e+00,  2.0000000e+00, -5.0000000e+00],\n",
       "        [ 2.0000000e+00, -2.0000000e+00,  7.0000000e+00],\n",
       "        [ 1.0000000e+00,  0.0000000e+00, -1.0000000e+00],\n",
       "        ...,\n",
       "        [ 1.3000000e+01, -2.0000000e+00, -7.0000000e+00],\n",
       "        [ 3.0000000e+00,  3.0000000e+00, -1.0000000e+00],\n",
       "        [ 4.0000000e+00,  0.0000000e+00,  6.6667112e-06]],\n",
       "\n",
       "       [[ 6.0000000e+00, -3.0000000e+00,  2.0000000e+00],\n",
       "        [ 0.0000000e+00,  5.0000000e+00, -7.0000000e+00],\n",
       "        [ 1.0000000e+00, -1.0000000e+00,  2.0000000e+00],\n",
       "        ...,\n",
       "        [ 1.1000000e+01, -9.0000000e+00,  3.0000000e+00],\n",
       "        [ 6.0000000e+00,  2.0000000e+00,  0.0000000e+00],\n",
       "        [ 4.0000000e+00,  6.6667112e-06, -1.3333378e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainimg=img_transform(xtrain)\n",
    "ytrainimg=img_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_final_x={}\n",
    "dict_final_y={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 159.6667890548706\n"
     ]
    }
   ],
   "source": [
    "n_examples=17*10\n",
    "now=time.time()\n",
    "while(n_examples):\n",
    "    while(uid in dict_final_x.keys()):\n",
    "        uid,xtrain,ytrain=randomTrainingExample(dict_df)\n",
    "    xtrain=np.array(xtrain)\n",
    "    ytrain=np.array(ytrain)\n",
    "    xtrain= xtrain.astype(np.float64)\n",
    "    ytrain= ytrain.astype(np.float64)\n",
    "    dict_final_x[uid]=xtrainimg\n",
    "    dict_final_y[uid]=ytrainimg\n",
    "    n_examples-=1\n",
    "then=time.time()\n",
    "print(\"Time Taken:\",then-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 100])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_final_y[uid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the custom dataloader\n",
    "def get_x(uid_list):\n",
    "    for i,uid in enumerate(uid_list):\n",
    "        x=dict_final_x[uid]\n",
    "        x=x.view(1,x.shape[0],x.shape[1],x.shape[2])\n",
    "        if(i==0):\n",
    "            out=x\n",
    "        else:\n",
    "            out = torch.cat((out, x), 0)\n",
    "    return out\n",
    "        \n",
    "def get_y(uid_list):\n",
    "    for i,uid in enumerate(uid_list):\n",
    "        y=dict_final_y[uid]\n",
    "        y=y.view(1,y.shape[0],y.shape[1],y.shape[2])\n",
    "        if(i==0):\n",
    "            out=y\n",
    "        else:\n",
    "            out = torch.cat((out, y), 0)\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(list(dict_final_x.keys()), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1492) must match the size of tensor b (1500) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-0903da0833af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# ===================forward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1492) must match the size of tensor b (1500) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img = get_x(data)\n",
    "        img_out=get_y(data)\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img_out)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "save_image(pic, './dc_img/image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([10, 1, 1500, 100])\n",
      "torch.Size([7, 1, 1500, 100])\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "for data in dataloader:\n",
    "    print(get_y(data).shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrainimg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainimg=xtrainimg.view(1,3,1500,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_tensor = torch.cat((xtrainimg, xtrainimg), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1500, 100])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
